{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "September 21st, 2020 <br/>Hannah Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition for the getNaverNewsLinks() function, which returns the list of all Naver News links for the input news publisher on the strike of the doctors through the input searchURL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNaverNewsLinks(searchURL, name):\n",
    "    \"\"\"A browser must be defined before running this function.\"\"\"\n",
    "    \n",
    "    browser.get(searchURL)\n",
    "    \n",
    "    # set news publisher\n",
    "    togglePublisherPreference(name)\n",
    "    \n",
    "    # scrape links from search engine results\n",
    "    mainArea = browser.find_element_by_css_selector('div.news.mynews.section._prs_nws')\n",
    "    allLinks = mainArea.find_elements_by_css_selector('ul li dl dd.txt_inline a')\n",
    "    allLinks = [x.get_attribute('href') for x in allLinks if x.text=='네이버뉴스']\n",
    "    nextPageButton = mainArea.find_elements_by_css_selector('div.paging a.next')\n",
    "    \n",
    "    while nextPageButton:\n",
    "        nextPageButton[0].click()\n",
    "        mainArea = browser.find_element_by_css_selector('div.news.mynews.section._prs_nws')\n",
    "        links = mainArea.find_elements_by_css_selector('ul li dl dd.txt_inline a')\n",
    "        links = [x.get_attribute('href') for x in links if x.text=='네이버뉴스']\n",
    "        allLinks.extend(links)\n",
    "        nextPageButton = mainArea.find_elements_by_css_selector('div.paging a.next')\n",
    "        \n",
    "    # reset news publisher\n",
    "    togglePublisherPreference(name)\n",
    "        \n",
    "    return allLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition for the togglePublisherPreference() function, which toggles the selection of the news publisher in the setting for the news source preference for the news publisher with the input name. This is a helper function used in the getNaverNewsLinks() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def togglePublisherPreference(name):\n",
    "    \"\"\"A helper function for the getNaverNewsLinks() function. A browser must be defined beforehand.\"\"\"\n",
    "    preferenceButton = [x for x in browser.find_elements_by_css_selector('li.menu > a.m') if (x.text=='언론사' or x.text=='출처선택')][0]\n",
    "    preferenceButton.click() # open box containing news publisher preference options\n",
    "    popUpBox = browser.find_element_by_css_selector('#_nx_option_media')\n",
    "    pubOption = popUpBox.find_elements_by_css_selector('ul.viewlst li')\n",
    "    for option in pubOption: # find option with desired news publisher\n",
    "        if option.find_element_by_css_selector('label').text == name:\n",
    "            pubOption = option.find_element_by_css_selector('input')\n",
    "            break\n",
    "    pubOption.click()\n",
    "    submitButton = popUpBox.find_element_by_css_selector('div.view_btn button._submit_btn')\n",
    "    submitButton.click() # submit preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the browser, the search URL containing the query for the search engine, a list of central news publishers, and the dictionary linksByPublisher, which contains all relevant Naver News links for each publisher. The list valid_types contains a list of numbers representing the type of article that provide the desired relevant search results. Prints out the number of article links obtained for each news publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경향신문 262\n",
      "국민일보 250\n",
      "내일신문 0\n",
      "동아일보 173\n",
      "문화일보 86\n",
      "서울신문 4191\n",
      "세계일보 398\n",
      "조선일보 245\n",
      "중앙일보 26678\n",
      "한겨레 2637\n",
      "한국일보 653\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Chrome(executable_path='/usr/local/bin/chromedriver')\n",
    "\n",
    "valid_types = [1, 3] # 1 for photo, 3 for paper\n",
    "\n",
    "publishers = ['경향신문', '국민일보', '내일신문', '동아일보', '문화일보', \n",
    "              '서울신문', '세계일보', '조선일보', '중앙일보', '한겨레', '한국일보']\n",
    "\n",
    "linksByPublisher = dict()\n",
    "\n",
    "searchLink = \"\"\"https://search.naver.com/search.naver?where=news\n",
    "&query=%22%EC%9D%98%EC%82%AC%22%20%22%ED%8C%8C%EC%97%85%22&sm=tab_opt&sort=0\n",
    "&photo={}&field=0&reporter_article=&pd=3&ds=2020.07.23&de=2020.10.20&docid=\n",
    "&nso=so%3Ar%2Cp%3Afrom20200723to20200920%2Ca%3Aall&mynews=1&refresh_start=0&related=0\"\"\"\n",
    "\n",
    "for publisher in publishers:\n",
    "    linksByPublisher[publisher] = []\n",
    "    for article_type in valid_types:\n",
    "        linksByPublisher[publisher].extend(getNaverNewsLinks(searchLink.format(article_type), publisher))\n",
    "    print(publisher, len(linksByPublisher[publisher]))\n",
    "    \n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition for the getHtmlDataAndInformation() function, which returns a tuple consisting of 1) the raw HTML data of the page and 2) a dictionary containing information about the article accessed through the input link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtmlDataAndInformation(link):\n",
    "    \"\"\"A browser must be defined before running this function.\"\"\"\n",
    "    browser.get(link)\n",
    "    time.sleep(0.5) # wait for page to load\n",
    "    \n",
    "    html = browser.page_source\n",
    "    information = dict()\n",
    "    \n",
    "    try:\n",
    "        # title area\n",
    "        titleArea = browser.find_element_by_css_selector('div.article_info')\n",
    "        information['title'] = titleArea.find_element_by_css_selector('#articleTitle').text\n",
    "\n",
    "        # convert text to datetime object\n",
    "        publishedTime = titleArea.find_element_by_css_selector('span.t11').text\n",
    "        publishedTime = publishedTime.replace('오전', 'AM')\n",
    "        publishedTime = publishedTime.replace('오후', 'PM')\n",
    "        information['datetime'] = datetime.datetime.strptime(publishedTime, '%Y.%m.%d. %p %I:%M')\n",
    "\n",
    "        numComments = ''.join(titleArea.find_element_by_css_selector('span.lo_txt').text.split(','))\n",
    "        information['num_comments'] = int(numComments) if numComments.isdigit() else 0\n",
    "\n",
    "        # get article content without links to irrelevant articles\n",
    "        information['content'] = '\\n'.join([x.strip() for x in browser.find_element_by_css_selector('#articleBodyContents').text.split('\\n') if not x.startswith('▶')])\n",
    "\n",
    "        # bottom area\n",
    "        bottomArea = browser.find_element_by_css_selector('#spiLayer')\n",
    "        # reactions\n",
    "        numReactions_good = int(''.join(bottomArea.find_element_by_css_selector('li.good span._count').text.split(',')))\n",
    "        numReactions_warm = int(''.join(bottomArea.find_element_by_css_selector('li.warm span._count').text.split(',')))\n",
    "        numReactions_sad = int(''.join(bottomArea.find_element_by_css_selector('li.sad span._count').text.split(',')))\n",
    "        numReactions_angry = int(''.join(bottomArea.find_element_by_css_selector('li.angry span._count').text.split(',')))\n",
    "        numReactions_want = int(''.join(bottomArea.find_element_by_css_selector('li.want span._count').text.split(',')))\n",
    "        information['reactions'] = {'good': numReactions_good, \n",
    "                                    'warm': numReactions_warm,\n",
    "                                    'sad': numReactions_sad, \n",
    "                                    'angry': numReactions_angry, \n",
    "                                    'want': numReactions_want\n",
    "                                   }\n",
    "        # recommends\n",
    "        numRecommends = ''.join(bottomArea.find_element_by_css_selector('em.u_cnt._count').text.split(','))\n",
    "        information['recommends'] = int(numRecommends) if numRecommends.isdigit() else 0\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        print(\"Failed to extract information from {}\".format(link))\n",
    "    \n",
    "    return (html, information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stores raw HTML data for all articles for each news publisher in the dictionary allRawHtmlData.\n",
    "Also stores article information for each article for each news publisher in the dictionary articleInfo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경향신문 262 complete\n",
      "Failed to extract information from https://sports.news.naver.com/news.nhn?oid=005&aid=0001357300\n",
      "국민일보 250 complete\n",
      "내일신문 0 complete\n",
      "동아일보 173 complete\n",
      "문화일보 86 complete\n",
      "서울신문 4191 complete\n",
      "세계일보 398 complete\n",
      "조선일보 245 complete\n",
      "중앙일보 26678 complete\n",
      "한겨레 2637 complete\n",
      "한국일보 653 complete\n"
     ]
    }
   ],
   "source": [
    "browser = webdriver.Chrome(executable_path='/usr/local/bin/chromedriver')\n",
    "\n",
    "allRawHtmlData = dict() # make json later\n",
    "articleInfo = dict()\n",
    "\n",
    "for publisher in linksByPublisher:\n",
    "    allRawHtmlData[publisher] = dict()\n",
    "    articleInfo[publisher] = dict()\n",
    "    newsLinks = linksByPublisher[publisher]\n",
    "    for newsLink in newsLinks:\n",
    "        data, info = getHtmlDataAndInformation(newsLink)\n",
    "        allRawHtmlData[publisher][newsLink] = data\n",
    "        articleInfo[publisher][newsLink] = info\n",
    "    print(publisher, len(newsLinks), \"complete\")\n",
    "\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stores variables for use in the other Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'publishers' (list)\n",
      "Stored 'articleInfo' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store publishers\n",
    "%store articleInfo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
